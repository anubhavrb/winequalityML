
\newpage

\section{Experiments}
\label{sec:expts}

The steps we followed while conducting our experiments are detailed below:
\begin{enumerate}
    \item All of our models were housed in separate files. Each model imported their respective model files from the \texttt{scikit-learn} library, and the \path{get_preprocessed_dataset} and \path{get_XY} functions from \path{winequality.py}. Some models made use of other functions that will be detailed later in this section. All models were run using a \path{run_model} function that we wrote to fetch the preprocessed dataset, train the model, and report the $R^2$ score for the model using the model's \texttt{score} function. All models were run $50$ times and the final $R^2$ scores reported in this paper are the average of $50$ runs.
    \item Initially, we ran all models using their default settings, and also using all the features in the dataset. The results we got served as our baseline results, so any time we modified our models, we could see if they did better or worse. After the initial step, we decided to add the \path{drop_outliers} function to the preprocessing step. Based on the graph of correlations between feature variables and quality scores, we observed a few outlier feature scores that were more than $3$ standard deviations away from the mean. The performance scores for all models improved in a small but consistent manner after these outliers were dropped.
    \item Next, we decided to tune hyperparameters to increase the performance of our models. We used \texttt{GridSearchCV} from the \texttt{scikit-learn} library to do this. \texttt{GridSearchCV} not only uses grid search to try different combinations of hyperparameters, but it also uses cross-validation when training and testing the models. For our SGD model, we varied \textbf{alpha}, a constant that multiplies the regularization term, over [0.00001, 0.00002, 0.00005, 0.0001]; we then varied \textbf{eta0}, the learning rate of the SGD model, over [0.001, 0.002, 0.005, 0.01]. For our ridge regression model, we varied \textbf{alpha} over [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]. For the LASSO model, we varied \textbf{alpha} over [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0]. Finally, for the SGD classifier model, we varied \textbf{eta0} over [0.001, 0.002, 0.005, 0.01], and \textbf{alpha} over [0.00001, 0.00002, 0.00005, 0.0001]. \texttt{GridSearchCV} resulted in increased scores for all the models.
    \item We were curious whether increasing the complexity of the model would improve the scores, so we decided to adopt the \texttt{PolynomialFeatures} function from the \path{scikit-learn} library. This feature generates new feature columns consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. After trying to increase degrees to $2$,$3$,and $4$, we observed that a polynomial degree of $2$ gave the best performance for all models, and scores started to deteriorate for models with degrees higher than $2$.
    \item Finally, we used feature selection/elimination in an effort to improve the performance of our models. Using the dataset, we generated a series of analyses using SPSSStatistics\footnote{A software we used to perform statistical analysis}on correlations between individual feature variables and quality scores. We then ranked correlations from highest to lowest for both red and white wine, and picked the top $8$ features with the highest correlations as our default selected features. However, it turned out that feature selection didn't have a significant effect on model performance, so we decided to use all the features while training our models.
\end{enumerate}

Our results will be discussed in the next section.
